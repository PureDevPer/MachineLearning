{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-1zcOVH538d",
        "colab_type": "text"
      },
      "source": [
        "# Recognizing Hand-Written Characters using Tensorflow\n",
        "\n",
        "\n",
        "*   Wooseok Kim\n",
        "*   James Schallert\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8jNgQUv5i3k",
        "colab_type": "text"
      },
      "source": [
        "# Setting Google Account / Google Drive "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8NjhJc2Hf_B",
        "colab_type": "text"
      },
      "source": [
        "When we run below code, it takes so long. Thus, we decide to use Google CoLab to save much time because Google CoLab offers decent GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FgRssuh0WKo",
        "colab_type": "code",
        "outputId": "9ab0a9a4-b84d-477a-fa66-d85a99b9f669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGkfd2QE4jge",
        "colab_type": "text"
      },
      "source": [
        "Connecting Google Account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h67HRmyM1h_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7OYGmch40uS",
        "colab_type": "text"
      },
      "source": [
        "Connecting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzxvopyZ22Bn",
        "colab_type": "code",
        "outputId": "859ed46c-96c9-467a-d22d-a3c58a3508d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "!ls -ltr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12\n",
            "drwxr-xr-x 1 root root 4096 Apr  4 20:20 sample_data\n",
            "-rw-r--r-- 1 root root 2494 Apr 26 00:52 adc.json\n",
            "drwxr-xr-x 2 root root 4096 Apr 26 00:52 drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIauWB8828Au",
        "colab_type": "code",
        "outputId": "4994196b-b33b-430d-ba69-6a555398ad44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-NtKwZw2_qR",
        "colab_type": "code",
        "outputId": "3fb31bb1-050c-4a2d-e9e1-2661f19a0e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd emnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/emnist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASJDP3qe3mGw",
        "colab_type": "code",
        "outputId": "d257ee3b-c09a-4ac6-fa86-9050a157f807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "!ls -ltr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 778000\n",
            "-rw-r--r-- 1 root root  36454802 Apr 25 00:06 emnist-balanced-test.csv\n",
            "-rw-r--r-- 1 root root 218619475 Apr 25 00:08 emnist-balanced-train.csv\n",
            "-rw-r--r-- 1 root root  77362729 Apr 25 00:09 emnist-digits-test.csv\n",
            "-rw-r--r-- 1 root root 464090839 Apr 25 00:15 emnist-digits-train.csv\n",
            "drwxr-xr-x 2 root root      4096 Apr 25 01:38 MNIST_data\n",
            "-rw-r--r-- 1 root root     19697 Apr 25 03:23 ensemble.ipynb\n",
            "-rw-r--r-- 1 root root    118367 Apr 26 00:53 MachineLearning.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUp0w41H7kd5",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "\n",
        "*   https://www.kaggle.com/crawford/emnist\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBRkycQ55Qml",
        "colab_type": "text"
      },
      "source": [
        "# Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQWPTTV47tJl",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtP2p-buH3Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spyHtuEmIplY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Import libraries\n",
        "*   Importing training data set and test data set using Pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAmxVQ28Iz99",
        "colab_type": "code",
        "outputId": "b8242725-02f3-43dc-b4a8-851cf955fb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "print('numTrain: ', numTrain)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "print('numTest: ', numTest)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "print('numClasses: ', numClasses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  112800\n",
            "numTest:  18800\n",
            "numClasses:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMjAEoe2JBvl",
        "colab_type": "text"
      },
      "source": [
        "We check how many training data set and test data set there are. Also, we are able to figure out how many different output there are. If we use emnist data set, there are 47 outputs including 10 digits, 26 letters, and 11 capital letters. If we use eMNIST digit data set, the number of classes is 10 because of only 10 digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoOZlOcWJsD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEPeBjGeLNQp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   train_data: Training data set without labels\n",
        "*   train_labels: Results of training data set\n",
        "*   test_data: Test data set without labels\n",
        "*   test_labels: Results of test data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1pqAtB_L-ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rteegmf5NJgC",
        "colab_type": "text"
      },
      "source": [
        "Use one-hot encoding\n",
        "\n",
        "*  Reference\n",
        "\n",
        "\n",
        ">* http://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
        ">* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVtQsqcXNlO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqow1toEOOQ1",
        "colab_type": "text"
      },
      "source": [
        "Training data and test data, which categorical variables are converted by one-hot encoding, are overwritten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwmyfQj5OuSZ",
        "colab_type": "code",
        "outputId": "207156ef-ebcf-4086-8883-491aecf961ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1xFsqFOxTL",
        "colab_type": "text"
      },
      "source": [
        "A training data randomly chosen are displayed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-yvut2hO5Ov",
        "colab_type": "code",
        "outputId": "a9b0c6b6-4c50-4716-a81a-45f422bac043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWM0iLyUO96N",
        "colab_type": "text"
      },
      "source": [
        "A training data which are randomly selected should be roated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyqDyjZG7BRh",
        "colab_type": "code",
        "outputId": "4af4b860-ea29-44e3-f446-13ddc68f8308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "#del train, test\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gqP8DmQRwXG8zcMkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HgJhGfNW-Tm",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhZcf_n8BEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4nuWSIf8EDd",
        "colab_type": "text"
      },
      "source": [
        "A given data have 28*28 image. So we need 784. Also, the data we are currently using has 47 classes, which includes 10 digits (0~9), 26 letters, and 11 capital letters.\n",
        "\n",
        "tf.placeholder(tf.float32, [None, 784]): In the code, none means we can give the data we want\n",
        "\n",
        "![alt text](http://1.bp.blogspot.com/-aluMK8npLy0/VSoWDBNNblI/AAAAAAAABc8/XC1sy_3rsjw/s1600/MNIST1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IubE8tK8DMq",
        "colab_type": "code",
        "outputId": "2f84eb74-169a-4b3d-9fd6-ac97f5249583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "W = tf.Variable(tf.random_normal([784, numClasses]))\n",
        "b = tf.Variable(tf.random_normal([numClasses]))\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJwkBwQmSwQh",
        "colab_type": "text"
      },
      "source": [
        "* W = tf.Variable(tf.random_normal([784, numClasses])): Input: 784, Output: numClasses, which indicates 47\n",
        "* b = tf.Variable(tf.random_normal([numClasses])): Y size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjRlon3i8JWh",
        "colab_type": "text"
      },
      "source": [
        "Multinomial Classfication\n",
        "$$\n",
        "W = \n",
        "\\begin{bmatrix}\n",
        "    w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
        "    w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    w_{n1} & w_{n2} & \\dots & w_{nn}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jTPkyZc8NCB",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "X = \n",
        "\\begin{bmatrix}\n",
        "    x_{1} \\\\\n",
        "    x_{2} \\\\\n",
        "    \\vdots \\\\\n",
        "    x_{n}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiSXEDQt8QSF",
        "colab_type": "text"
      },
      "source": [
        "Softmax\n",
        "$$\n",
        "S(y_i) = \\frac{e^y_i} {\\sum_j e^y_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ll2YqrF8Toz",
        "colab_type": "text"
      },
      "source": [
        "In sum, let's suppose y = XW\n",
        "$$\n",
        "XW = y  => \\frac{e^y_i} {\\sum_j e^y_i} => Probabilities\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlBbfdP28WS8",
        "colab_type": "text"
      },
      "source": [
        "This is Softmax on Tensorflow. \n",
        "softmax(tf.matmul(X, W) + b).\n",
        "So, tf.nn.softmax computes softmax activations. We can say that \n",
        "$$\n",
        "softmax = \\frac{exp(logits)} {reduceSum(exp(logits), dim)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL7AQl0n8ZFg",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "cost(W) = -\\frac {1} {m} \\sum ylog(H(x)) + (1-y)log(1-H(x)) \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK_FGNU58dFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS_Q1lo28eNu",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "W = W -\\alpha \\frac{\\partial}{\\partial W} cost(W)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_elE8Owz8hEr",
        "colab_type": "code",
        "outputId": "2f5e30e5-08b4-4f73-db05-0f28e1ffecdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVcDBGdX8lwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VcyZU4iTbSN",
        "colab_type": "text"
      },
      "source": [
        "* is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1)): Hypothesis value and Y value are compared\n",
        "* accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)): Calculate average value of is_correct. This is accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiaCVRVU8o0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "num_iterations = int( numTrain / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oovFCK78pi-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   epoch = one forward pass and one backward pass of all the training examples\n",
        "*   batch size = Number of training examples in one forward/backward pass. The higher the batch size is, the more memory space you'll need is\n",
        "*  Number of iterations = number of passes, each pass using batch size number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
        "[5]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Akmm8JSFhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwIMo_qgSGK6",
        "colab_type": "text"
      },
      "source": [
        "Training data can read 100 data at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd8RHFRMUpEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9KL1DEuUpr4",
        "colab_type": "text"
      },
      "source": [
        "Calculate cost value using Optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NJN_NwhSbTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: test_data, Y: test_labels}\n",
        "        ),\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JjmrckMSb4R",
        "colab_type": "text"
      },
      "source": [
        "We use test data set which we haven't used before. So we evaluate test data set. Finally, we can calculate accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTe1uEEmVemt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8HXoz-0VfEh",
        "colab_type": "text"
      },
      "source": [
        "1. Get Random value\n",
        "2. Read one of test labels using argmax because we used one-hot encoding\n",
        "3. For prediction, we will run hypothesis. And then, we will get the image which are the same as hypothesis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yydob5C81Mg",
        "colab_type": "code",
        "outputId": "c11a34c8-8bf7-4f9b-e5dd-6e5fb397872e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: test_data, Y: test_labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Cost: 7.831361669\n",
            "Epoch: 0002, Cost: 3.635979679\n",
            "Epoch: 0003, Cost: 2.910056583\n",
            "Epoch: 0004, Cost: 2.549824970\n",
            "Epoch: 0005, Cost: 2.325167478\n",
            "Epoch: 0006, Cost: 2.168853015\n",
            "Epoch: 0007, Cost: 2.052359234\n",
            "Epoch: 0008, Cost: 1.961268801\n",
            "Epoch: 0009, Cost: 1.887505654\n",
            "Epoch: 0010, Cost: 1.826184638\n",
            "Epoch: 0011, Cost: 1.774160550\n",
            "Epoch: 0012, Cost: 1.729304836\n",
            "Epoch: 0013, Cost: 1.690116639\n",
            "Epoch: 0014, Cost: 1.655501113\n",
            "Epoch: 0015, Cost: 1.624637229\n",
            "Epoch: 0016, Cost: 1.596895715\n",
            "Epoch: 0017, Cost: 1.571785485\n",
            "Epoch: 0018, Cost: 1.548917508\n",
            "Epoch: 0019, Cost: 1.527979174\n",
            "Epoch: 0020, Cost: 1.508715988\n",
            "Learning finished\n",
            "Accuracy:  0.6129255\n",
            "Label:  [24]\n",
            "Prediction:  [0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJpELzRmXGMB",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFHQ71sViGA",
        "colab_type": "text"
      },
      "source": [
        "### Alphabet / Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU_pUNrn6KSY",
        "colab_type": "code",
        "outputId": "c44240c5-3101-4298-8619-6294e5036445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([784, numClasses]))\n",
        "b = tf.Variable(tf.random_normal([numClasses]))\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: test_data, Y: test_labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Cost: 7.811212356\n",
            "Epoch: 0002, Cost: 3.577890929\n",
            "Epoch: 0003, Cost: 2.874251862\n",
            "Epoch: 0004, Cost: 2.526106676\n",
            "Epoch: 0005, Cost: 2.309650495\n",
            "Epoch: 0006, Cost: 2.159337446\n",
            "Epoch: 0007, Cost: 2.047290485\n",
            "Epoch: 0008, Cost: 1.959520055\n",
            "Epoch: 0009, Cost: 1.888236626\n",
            "Epoch: 0010, Cost: 1.828750604\n",
            "Epoch: 0011, Cost: 1.778055745\n",
            "Epoch: 0012, Cost: 1.734127743\n",
            "Epoch: 0013, Cost: 1.695548977\n",
            "Epoch: 0014, Cost: 1.661293361\n",
            "Epoch: 0015, Cost: 1.630596669\n",
            "Epoch: 0016, Cost: 1.602875596\n",
            "Epoch: 0017, Cost: 1.577675840\n",
            "Epoch: 0018, Cost: 1.554637091\n",
            "Epoch: 0019, Cost: 1.533469000\n",
            "Epoch: 0020, Cost: 1.513934543\n",
            "Learning finished\n",
            "Accuracy:  0.6085106\n",
            "Label:  [36]\n",
            "Prediction:  [35]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOEHgvo/kwoOaYkQq0QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRHpCuVGVkGi",
        "colab_type": "text"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1sgBUU4VphW",
        "colab_type": "code",
        "outputId": "f80ef774-ed38-4f3f-9d27-2b3a490c9d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "#test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([784, numClasses]))\n",
        "b = tf.Variable(tf.random_normal([numClasses]))\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: test_data, Y: test_labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Cost: 12.294879703\n",
            "Epoch: 0002, Cost: 7.675846588\n",
            "Epoch: 0003, Cost: 5.530997338\n",
            "Epoch: 0004, Cost: 4.266942058\n",
            "Epoch: 0005, Cost: 3.466805372\n",
            "Epoch: 0006, Cost: 2.930163783\n",
            "Epoch: 0007, Cost: 2.553369329\n",
            "Epoch: 0008, Cost: 2.277840718\n",
            "Epoch: 0009, Cost: 2.068908990\n",
            "Epoch: 0010, Cost: 1.905354305\n",
            "Epoch: 0011, Cost: 1.773776403\n",
            "Epoch: 0012, Cost: 1.665476663\n",
            "Epoch: 0013, Cost: 1.574621240\n",
            "Epoch: 0014, Cost: 1.497164249\n",
            "Epoch: 0015, Cost: 1.430218704\n",
            "Epoch: 0016, Cost: 1.371676122\n",
            "Epoch: 0017, Cost: 1.319963597\n",
            "Epoch: 0018, Cost: 1.273884969\n",
            "Epoch: 0019, Cost: 1.232514620\n",
            "Epoch: 0020, Cost: 1.195125107\n",
            "Learning finished\n",
            "Accuracy:  0.7697\n",
            "Label:  [2]\n",
            "Prediction:  [6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB2tXU1N9v9y6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jv4hGfTGxPs",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZc9QXqoXYd0",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmolKQK5PczQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrfAQzKpPkvl",
        "colab_type": "text"
      },
      "source": [
        "The process of importing training data set and test data set is the same as softmax algorithm above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE0gexjZXcYV",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyKR2JoYzH6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([512, 512]))\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([512, 512]))\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.Variable(tf.random_normal([512, numClasses]))\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxU3AEUex8yi",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
        "\n",
        "The figure[2] shows a regular 3-layer Neural Network. For the project, we are using 4-layer neural network. In other words, input layer in the code above is placeholders of X and Y. Input layer, which is a placeholder of X, is 784 because image size is 28*28. Output is 47, Y placeholder.\n",
        "\n",
        "We have 3 hidden layers. First, hidden layer 1 (W1) has 784 inputs and 512 outputs. Hidden layers 2 and 3 have 512 inputs and 512 outpus. Since the output of hidden layer 1 is 512, we should have 512 inputs in hidden layer 2. Finally, we will get 47 outputs including 10 digits, 26 letters, and 11 capital letters. We use ReLU to calculate each layer.\n",
        "\n",
        "\n",
        "![alt text](http://www.cs.albany.edu/~wooseok/536/NN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVViUQN_vIRQ",
        "colab_type": "text"
      },
      "source": [
        "ReLU: Rectified Linear Unit [4]\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XnVCZ6PGczY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6KnCENFGdUm",
        "colab_type": "text"
      },
      "source": [
        "Computes softmax cross entropy between hypothesis(logits) and outputs(Y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HViLFWmp2nqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-12wIhM1EGl",
        "colab_type": "text"
      },
      "source": [
        "In softmax algoritm, we use GradientDescentOptimizer in order for train. However, in neural network, we use AdamOptimizer. \n",
        "\n",
        "Here's interesting comparison in terms of optimization. The below site compares below optimazation algorithms\n",
        "\n",
        "\n",
        "*   SGD\n",
        "*   Momentum\n",
        "*   NAG\n",
        "*   Adagrad\n",
        "*   Adadelta\n",
        "*   RMSprop\n",
        "\n",
        "http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu5DJ8bV2oGK",
        "colab_type": "text"
      },
      "source": [
        "A reason why we use AdamOptimizer is that Adam shows the best optimization performance[3].\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neJ_ozfW5opz",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow offers Adam optimization library. So we can use easily the adam optimization algorithm. \n",
        "https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmWq3A0fKTfn",
        "colab_type": "text"
      },
      "source": [
        "Lastly, we will train and evaluate our training using accuracy. The code is the same as Softmax algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kj2GNhkKznN",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDNewPKfUEuM",
        "colab_type": "text"
      },
      "source": [
        "### Alphabet / Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAkMr5lpGmOG",
        "colab_type": "code",
        "outputId": "0ef2c0f0-6c7f-43ca-8b71-94bff8a448d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([512, 512]))\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([512, 512]))\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.Variable(tf.random_normal([512, numClasses]))\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels}))\n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  112800\n",
            "Epoch: 0001, Cost: 8642.529711351\n",
            "Epoch: 0002, Cost: 2481.713991206\n",
            "Epoch: 0003, Cost: 1461.960102920\n",
            "Epoch: 0004, Cost: 983.205817202\n",
            "Epoch: 0005, Cost: 699.275430531\n",
            "Epoch: 0006, Cost: 514.055722013\n",
            "Epoch: 0007, Cost: 382.135182009\n",
            "Epoch: 0008, Cost: 290.110750185\n",
            "Epoch: 0009, Cost: 223.293022807\n",
            "Epoch: 0010, Cost: 173.172536803\n",
            "Epoch: 0011, Cost: 138.131260886\n",
            "Epoch: 0012, Cost: 109.862700687\n",
            "Epoch: 0013, Cost: 90.336773301\n",
            "Epoch: 0014, Cost: 74.091697410\n",
            "Epoch: 0015, Cost: 63.775216297\n",
            "Learning finished\n",
            "Accuracy:  0.6993085\n",
            "Label:  [0]\n",
            "Prediction:  [24]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfRuP1TFUIeE",
        "colab_type": "text"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgLSNG8qUKh2",
        "colab_type": "code",
        "outputId": "e22748ec-a7bf-4c84-a606-e50ea6f02dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "#test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([512, 512]))\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([512, 512]))\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.Variable(tf.random_normal([512, numClasses]))\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels}))\n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  240000\n",
            "Epoch: 0001, Cost: 1254.097743122\n",
            "Epoch: 0002, Cost: 246.871940774\n",
            "Epoch: 0003, Cost: 137.709690004\n",
            "Epoch: 0004, Cost: 86.881786227\n",
            "Epoch: 0005, Cost: 59.710167026\n",
            "Epoch: 0006, Cost: 44.622771997\n",
            "Epoch: 0007, Cost: 35.687775417\n",
            "Epoch: 0008, Cost: 27.622199425\n",
            "Epoch: 0009, Cost: 24.814995090\n",
            "Epoch: 0010, Cost: 21.613631717\n",
            "Epoch: 0011, Cost: 19.230210329\n",
            "Epoch: 0012, Cost: 17.108183406\n",
            "Epoch: 0013, Cost: 15.746177453\n",
            "Epoch: 0014, Cost: 14.488535248\n",
            "Epoch: 0015, Cost: 13.864103281\n",
            "Learning finished\n",
            "Accuracy:  0.9823\n",
            "Label:  [8]\n",
            "Prediction:  [8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8gNBUX4gKMoPBEX5gaAoPxDU/wMtRQSSjsJufwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TO-ibUcHGgF",
        "colab_type": "text"
      },
      "source": [
        "# Xavier Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BiJoJrtFOBk",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVrltLF8Ff9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxndcFu5FQnt",
        "colab_type": "text"
      },
      "source": [
        "The process of importing training data set and test data set is the same as softmax algorithm above. The initial value at first is very important to train the data set. That is, we should make sure the weights are just right, not too small, and not too big."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTMKxoTuGGzj",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_h9Olg3LRuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdCQc5TLUdH",
        "colab_type": "text"
      },
      "source": [
        "Basic algorithm is the same as Neural Network above. A difference between Xavier algorithm  and Neural Network is an initialization value [6].  Input and output are randomly provided like below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyM8uGvqPlKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.randn(input, output)/np.sqrt(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp1BpT74QURV",
        "colab_type": "text"
      },
      "source": [
        "Google implemented prettytensor for Xavier initialization[7]. However, fortunately, tensorflow offers Xavier initialization library. So we use the library in hidden layer 1, 2, and 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS1qPQxRQhJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd7LUdpsRY9f",
        "colab_type": "text"
      },
      "source": [
        "Lastly, we use identical cost and Adam optimization like neural networks and then train and evaluate our training using accuracy. The code is the same as above.\n",
        "\n",
        "Cost values of Xavier are much lower than cost values of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_exD8GHLjU1",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d23I14ZSj7z",
        "colab_type": "text"
      },
      "source": [
        "### Alphabet / Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYPlzVdeHKGZ",
        "colab_type": "code",
        "outputId": "182fa713-8042-43d8-ac3e-70f384bbdb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels}))\n",
        "    \n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "numTrain:  112800\n",
            "Epoch: 0001, Cost: 0.922173137\n",
            "Epoch: 0002, Cost: 0.501473968\n",
            "Epoch: 0003, Cost: 0.411888543\n",
            "Epoch: 0004, Cost: 0.357318249\n",
            "Epoch: 0005, Cost: 0.319102287\n",
            "Epoch: 0006, Cost: 0.288661301\n",
            "Epoch: 0007, Cost: 0.262170818\n",
            "Epoch: 0008, Cost: 0.242621696\n",
            "Epoch: 0009, Cost: 0.229773155\n",
            "Epoch: 0010, Cost: 0.212708886\n",
            "Epoch: 0011, Cost: 0.199395352\n",
            "Epoch: 0012, Cost: 0.196674012\n",
            "Epoch: 0013, Cost: 0.183474308\n",
            "Epoch: 0014, Cost: 0.173996904\n",
            "Epoch: 0015, Cost: 0.170004412\n",
            "Learning finished\n",
            "Accuracy:  0.8368085\n",
            "Label:  [6]\n",
            "Prediction:  [6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxojUkCfSmgM",
        "colab_type": "text"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXXw_mSjSnqG",
        "colab_type": "code",
        "outputId": "cc33a501-aeb8-4739-90e3-3f87d5ffeebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "#test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels}))\n",
        "    \n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  240000\n",
            "Epoch: 0001, Cost: 0.112610524\n",
            "Epoch: 0002, Cost: 0.047038741\n",
            "Epoch: 0003, Cost: 0.035235770\n",
            "Epoch: 0004, Cost: 0.028168916\n",
            "Epoch: 0005, Cost: 0.023147189\n",
            "Epoch: 0006, Cost: 0.020137726\n",
            "Epoch: 0007, Cost: 0.018170127\n",
            "Epoch: 0008, Cost: 0.016829032\n",
            "Epoch: 0009, Cost: 0.015006357\n",
            "Epoch: 0010, Cost: 0.014536648\n",
            "Epoch: 0011, Cost: 0.013235325\n",
            "Epoch: 0012, Cost: 0.012501067\n",
            "Epoch: 0013, Cost: 0.012648829\n",
            "Epoch: 0014, Cost: 0.011626684\n",
            "Epoch: 0015, Cost: 0.010933093\n",
            "Learning finished\n",
            "Accuracy:  0.988775\n",
            "Label:  [8]\n",
            "Prediction:  [8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+Ymc+n+c0B8UYKaQCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgIWz08HQc-",
        "colab_type": "text"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBNAIynDFSYw",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-mnXSLGFzO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cpyt-4tFT9Q",
        "colab_type": "text"
      },
      "source": [
        "The process of importing training data set and test data set is the same as above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0iQiAzqF18b",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-NjScajRwL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropout (keep_prob) rate\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSCYCAxkR7Cv",
        "colab_type": "text"
      },
      "source": [
        "We use dropout like below[8]. \n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*tMAwd9AC4f8YFW5RiMUNvA.png)\n",
        "\n",
        "\n",
        "Figure (a) shows general neural networks. Figure (b) indicates dropout which means that nodes are randomly disconnected. So, we should set randomly some neurons to zero in the forword pass. This avoids overfitting because networks can memorize training data set while training. We use dropout when we train data set. However, when we test data set, we should not apply dropout to the networks. If we use dropout during the test, we can't evaluate our training dataset well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGNXt9dIqeL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1mZJYSxqkFW",
        "colab_type": "text"
      },
      "source": [
        "In order to implement dropout, we can add only one line due to tensorflow library. In the code, input is L1/L2/L3 and output is also L1/L2/L3. keep_prob means how much networks keep. According to what we study, 50%~70% of networks are kept while training.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3srTn4Pso6f",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ZE-3fcsxuE",
        "colab_type": "text"
      },
      "source": [
        "Training code is very similar to neural networks and Xavier except for keep_prob. Since we use dropout, we should add keep_prob when we train and test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep5URdQPrdvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YchZNd6FrjRP",
        "colab_type": "text"
      },
      "source": [
        "We would like to keep 70% of networks while training. Therefore, keep_prob: 0.7 in the code above during training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMRo0cqvsr6s",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmdsYbBJtGJN",
        "colab_type": "text"
      },
      "source": [
        "Without keep_prob, the code is almost identical with neural networks and Xavier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyd8wiHsuam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model using test sets\n",
        "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels, keep_prob: 1}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia6vBjYVstqB",
        "colab_type": "text"
      },
      "source": [
        "As we discussed above, we need to keep 100% of networks while testing even if we apply dropout during training. Therefore, keep_prob: 1 which means that we will not drop any networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMYGHLdR8oq",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm2IZnCGRpPZ",
        "colab_type": "text"
      },
      "source": [
        "### Alphabet/Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tduz6PcHSO5",
        "colab_type": "code",
        "outputId": "b1d07752-dc96-44fc-d703-3d5050694524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels, keep_prob: 1}))\n",
        "    \n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1], keep_prob: 1}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  112800\n",
            "Epoch: 0001, Cost: 1.251659688\n",
            "Epoch: 0002, Cost: 0.728741870\n",
            "Epoch: 0003, Cost: 0.641459946\n",
            "Epoch: 0004, Cost: 0.590681597\n",
            "Epoch: 0005, Cost: 0.561483818\n",
            "Epoch: 0006, Cost: 0.539457501\n",
            "Epoch: 0007, Cost: 0.522869468\n",
            "Epoch: 0008, Cost: 0.505910995\n",
            "Epoch: 0009, Cost: 0.499571343\n",
            "Epoch: 0010, Cost: 0.490593512\n",
            "Epoch: 0011, Cost: 0.480654343\n",
            "Epoch: 0012, Cost: 0.479461333\n",
            "Epoch: 0013, Cost: 0.472976663\n",
            "Epoch: 0014, Cost: 0.461963027\n",
            "Epoch: 0015, Cost: 0.455598501\n",
            "Learning finished\n",
            "Accuracy:  0.84957445\n",
            "Label:  [3]\n",
            "Prediction:  [3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wOYEvZILhgw9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh1Gj3IEon3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN3v-CjKRsBQ",
        "colab_type": "text"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di5_jH6rRtbb",
        "colab_type": "code",
        "outputId": "e9fed4cf-441c-48ab-f7c9-f44e0cf9d251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "#test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "\ttrain_data[rand].reshape([28, 28]), \n",
        "\tcmap='Greys'\n",
        "\t)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([numClasses]))\n",
        "hypothesis = tf.matmul(L3, W4) + b4\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels, keep_prob: 1}))\n",
        "    \n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: test_data[r : r + 1], keep_prob: 1}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  240000\n",
            "Epoch: 0001, Cost: 0.175710808\n",
            "Epoch: 0002, Cost: 0.081280281\n",
            "Epoch: 0003, Cost: 0.067423555\n",
            "Epoch: 0004, Cost: 0.059984454\n",
            "Epoch: 0005, Cost: 0.056014791\n",
            "Epoch: 0006, Cost: 0.051717673\n",
            "Epoch: 0007, Cost: 0.050206302\n",
            "Epoch: 0008, Cost: 0.046855801\n",
            "Epoch: 0009, Cost: 0.044953876\n",
            "Epoch: 0010, Cost: 0.043244649\n",
            "Epoch: 0011, Cost: 0.044239188\n",
            "Epoch: 0012, Cost: 0.043146146\n",
            "Epoch: 0013, Cost: 0.040557425\n",
            "Epoch: 0014, Cost: 0.040846657\n",
            "Epoch: 0015, Cost: 0.040241697\n",
            "Learning finished\n",
            "Accuracy:  0.990575\n",
            "Label:  [0]\n",
            "Prediction:  [0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBxJF+YFEUX4gUZQfSBTlBxJF+YFE/T9I7Tg1VlfDbwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNlEmimnHeik",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtHBAa7BFVio",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M8-y7VUF6Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27-GVT8WFXGa",
        "colab_type": "text"
      },
      "source": [
        "The process of importing training data set and test data set is the same as softmax algorithm above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-xbsjNeF7iT",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEDQiRyiDbiw",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://cs231n.github.io/assets/cnn/convnet.jpeg)\n",
        "\n",
        "* Input: Our input is 28x28 image. \n",
        "* CONV layer: The layer will compute the output which are connected to local regions in the input\n",
        "* RELU layer: We use ReLU in order to apply an elementwise activation function\n",
        "* POOL layer: The layer will make a smaller image than original image by performing a downsampling operation\n",
        "* FC layer: FC stands for Fully Connected. The layer will compute the classes\n",
        "[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JouTf-EkGXze",
        "colab_type": "text"
      },
      "source": [
        "We use below CNN structure [9]\n",
        "![alt text](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8tvXheJHNQ7",
        "colab_type": "text"
      },
      "source": [
        "### Input layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNYA4VaeH3_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjwjdDih7PLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_img = tf.reshape(X, [-1, 28, 28, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OtxOMDG7Pu5",
        "colab_type": "text"
      },
      "source": [
        "* -1 indicates tensorflow decides the number of images.\n",
        "* Image size is 28 x 28. \n",
        "* color is 1. In other words, it is black and white. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE7Lt6EeH_7Y",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional layer1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BxUukZo7g4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIRLwWV87icD",
        "colab_type": "text"
      },
      "source": [
        "A filter size is 3x3 and color is the same as image size. And we will use 32 filters, which are different images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1dxj9cf7wK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (28, 28, 32)\n",
        "tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNWrEC4m7wuN",
        "colab_type": "text"
      },
      "source": [
        "1. The filter, which is 1x1, will move to 1 step. That is, strides tells how many steps can move at once.\n",
        "2. padding='SAME': Input image size and output image size are identical regardless of image size.\n",
        "3. After convolution 2d is done, image size is 28x28 like original image size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJO_G620IpI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L1 = tf.nn.relu(L1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhz1FPpSIpvE",
        "colab_type": "text"
      },
      "source": [
        "We use ReLU after convoluational layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXy9GjuhIUgV",
        "colab_type": "text"
      },
      "source": [
        "### Pooling layer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-L9D3LU8Mn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (14, 14, 32)\n",
        "tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOp_TK7N8OrM",
        "colab_type": "text"
      },
      "source": [
        "Input is L1 which is previous data set. Kernel size is 2x2 and filter size is also 2x2. Since strides is 2x2, original images will decrease. For instance, the image is 28x28 and modified image is 14x14 due to the stride, which is the filter. So, we will have 32 different images after max-pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb1fHa3AJEOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0JR69_rJErt",
        "colab_type": "text"
      },
      "source": [
        "Last step is to use dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9B-_i58JMLN",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJBqTE11JUiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# L2 Img Input shape=(n, 14, 14, 32)\n",
        "# Conv = (n, 14, 14, 64)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmlH5t7AJPa_",
        "colab_type": "text"
      },
      "source": [
        "Since we have 32 different images after pooling layer 1, we should use 32 colors as an input. Also, we increases the number of filters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdvJ3QCUJbzA",
        "colab_type": "text"
      },
      "source": [
        "### Pooling layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pOqdlpNJiK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pool = (n, 7, 7, 64)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5U0gXHIJgMW",
        "colab_type": "text"
      },
      "source": [
        "It is the same as pooling layer 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3mHLXMmKF7N",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional layer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKam4KHaKH7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# L3 Img Input shape=(n, 7, 7, 64)\n",
        "# Conv = (n, 7, 7, 128)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZp63AItKQEW",
        "colab_type": "text"
      },
      "source": [
        "Like convolutional layer 3, after pooling layer 2, there are 64 different images. So, our input should be 64 and we increases the number of filters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJ0lRanKg5F",
        "colab_type": "text"
      },
      "source": [
        "### Pooling layer 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd6lQz6SKoSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pool = (n, 4, 4, 128)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzU9JiRRKjNQ",
        "colab_type": "text"
      },
      "source": [
        "It is the same as pooling layer 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zev56i9LmKB",
        "colab_type": "text"
      },
      "source": [
        "### Locally Connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch2iFSLALosD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape = (n, 4 * 4 * 128) \n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tylmdPLszq",
        "colab_type": "text"
      },
      "source": [
        "Before our image gets into fully connected layer, we need to reshape our image. In convolutional layer 3, we have 4x4 and 128 different images after pooling. The reason why there are 128 different images is the number of filters in convolutional layer 3. In order to reshape array size, we should multiply 4x4x128. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aijEb9CULbqO",
        "colab_type": "text"
      },
      "source": [
        "### Fully Connected layer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpTG_oNDA3UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W4 = tf.get_variable(\"W4\", shape=[128*4*4, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4)+b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTToJZLXA3-R",
        "colab_type": "text"
      },
      "source": [
        "This is first fully connected layer. Output of array shape will be 625. And then, we use ReLU and dropout such as convolutional layer and pooling layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK17MIv1MKvq",
        "colab_type": "text"
      },
      "source": [
        "### Fully Connected layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGz-rsRcDMpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W5 = tf.get_variable(\"W5\", shape=[625, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([numClasses]))\n",
        "logits = tf.matmul(L4, W5) + b5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2e2pg35DNRN",
        "colab_type": "text"
      },
      "source": [
        "Lastly, our real output should be the number of classes, which is 47."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWSE6b4tNuHZ",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMs3yOolNv9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AU2uuPPNwgm",
        "colab_type": "text"
      },
      "source": [
        "Like we discusses above dropout algorithm, when we train data set, we would like to keep 70% of networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMzrttGSN8MU",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzX-MHPTN9lR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.run(tf.argmax(logits, 1), feed_dict={X: test_data[r : r + 1], keep_prob: 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iucMnqVxOD91",
        "colab_type": "text"
      },
      "source": [
        "However, we need to use all the networks when we test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxJchhAmDXXS",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAGV2x2TPwvH",
        "colab_type": "text"
      },
      "source": [
        "### Alphabet/Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At3ckEl7HgeK",
        "colab_type": "code",
        "outputId": "9f34b574-a24c-47c2-b70e-b91c3947203a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
        "\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# L1 Img Input shape=(n, 28, 28, 1)\n",
        "# Conv = (n, 28, 28, 32)\n",
        "# Pool = (n, 14, 14, 32)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 Img Input shape=(n, 14, 14, 32)\n",
        "# Conv = (n, 14, 14, 64)\n",
        "# Pool = (n, 7, 7, 64)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "# L3 Img Input shape=(n, 7, 7, 64)\n",
        "# Conv = (n, 7, 7, 128)\n",
        "# Pool = (n, 4, 4, 128)\n",
        "# Reshape = (n, 4 * 4 * 128) \n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 Fully Connected (FC) 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128*4*4, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4)+b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final Fully Connected (FC) 625 inputs -> numClasses outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([numClasses]))\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels, keep_prob: 1}))\n",
        "    \n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(logits, 1), feed_dict={X: test_data[r : r + 1], keep_prob: 1}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numTrain:  112800\n",
            "Epoch: 0001, Cost: 1.059922579\n",
            "Epoch: 0002, Cost: 0.515719846\n",
            "Epoch: 0003, Cost: 0.449088449\n",
            "Epoch: 0004, Cost: 0.412468898\n",
            "Epoch: 0005, Cost: 0.390916076\n",
            "Epoch: 0006, Cost: 0.373371925\n",
            "Epoch: 0007, Cost: 0.359837109\n",
            "Epoch: 0008, Cost: 0.349027968\n",
            "Epoch: 0009, Cost: 0.336722072\n",
            "Epoch: 0010, Cost: 0.331467421\n",
            "Epoch: 0011, Cost: 0.325443666\n",
            "Epoch: 0012, Cost: 0.317864727\n",
            "Epoch: 0013, Cost: 0.311824368\n",
            "Epoch: 0014, Cost: 0.309578560\n",
            "Epoch: 0015, Cost: 0.303445078\n",
            "Learning finished\n",
            "Accuracy:  0.89090425\n",
            "Label:  [37]\n",
            "Prediction:  [37]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADy1JREFUeJzt3WuMVHWax/Hfw2U0MhMCS9Mil+1x\nYrzEuMxaAjpgZh2ZgCHBeWNokoFJDMwLTJxkXmjYeItGzWZnRqOGpEECriOMZiCSaJxxyUbtZEMo\nCONlXIVFJgNyaeLIRTTcnn3Rh9lWu/6nrNup7uf7STpddZ46VU+f7l+fqvrXOX9zdwGIZ0TRDQAo\nBuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUqFY+2IQJE7yrq6uVDwmEsm/fPh09etSquW1d\n4TezeZKelDRS0hp3fzx1+66uLpXL5XoeEkBCqVSq+rY1P+03s5GSnpE0X9I1krrN7Jpa7w9Aa9Xz\nmn+GpD3uvtfdT0vaKGlhY9oC0Gz1hH+ypL8OuL4/W/YlZrbczMpmVu7r66vj4QA0UtPf7Xf3Hncv\nuXupo6Oj2Q8HoEr1hP+ApKkDrk/JlgEYAuoJ/3ZJV5jZd83sW5IWSdrSmLYANFvNQ33uftbM7pL0\nB/UP9a119/ca1hlaot4zOZlVNaSMNlTXOL+7vyrp1Qb1AqCF+HgvEBThB4Ii/EBQhB8IivADQRF+\nIKiWHs+P1jt//nyyvmnTpmT92LFjyfqSJUuS9dGjRyfrKA57fiAowg8ERfiBoAg/EBThB4Ii/EBQ\nDPUNc729vcn6fffdl6znnXpt8uSvnbntS+bNm5esozjs+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4g\nKMb5h4HUYbsPPfRQct0PP/wwWZ81a1ayft111yXraF/s+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4g\nqLrG+c1sn6QTks5JOuvupUY0hW8mdXrtbdu2JdfNm6J78eLFyXpnZ2eyjvbViA/5/Iu7H23A/QBo\nIZ72A0HVG36X9Ecz22FmyxvREIDWqPdp/2x3P2BmEyW9bmb/4+5vDrxB9k9huSRNmzatzocD0Ch1\n7fnd/UD2/YikzZJmDHKbHncvuXupo6OjnocD0EA1h9/MxpjZdy5clvRjSe82qjEAzVXP0/5OSZvN\n7ML9vODurzWkKwBNV3P43X2vpH9qYC+oIG+a7Q8++KBi7YsvvqjrscePH5+sZ//8MQQx1AcERfiB\noAg/EBThB4Ii/EBQhB8IilN3DwHHjx9P1leuXFmxljdMeMkllyTreVNsjxjB/mOo4jcHBEX4gaAI\nPxAU4QeCIvxAUIQfCIrwA0Exzj8EnDhxIlk/cOBAzfd9ww03JOtjx46t+b7R3tjzA0ERfiAowg8E\nRfiBoAg/EBThB4Ii/EBQjPMPAW+99VaynhrnHzUq/SvmeP24+M0CQRF+ICjCDwRF+IGgCD8QFOEH\ngiL8QFC54/xmtlbSAklH3P3abNl4Sb+T1CVpn6Q73P1vzWtzeMs7t/6aNWuS9VOnTlWs5Z2Xf9q0\nack6hq9q9vzrJH31kyD3Strq7ldI2ppdBzCE5Ibf3d+U9MlXFi+UtD67vF7S7Q3uC0CT1fqav9Pd\nD2aXD0nqbFA/AFqk7jf83N0leaW6mS03s7KZlfv6+up9OAANUmv4D5vZJEnKvh+pdEN373H3kruX\nOjo6anw4AI1Wa/i3SFqaXV4q6eXGtAOgVXLDb2YbJP23pCvNbL+Z3SnpcUlzzWy3pFuz6wCGkNxx\nfnfvrlD6UYN7QRPkjePPnj27RZ2g3fAJPyAowg8ERfiBoAg/EBThB4Ii/EBQnLq7DeSdHnvu3LnJ\nem9vb8Xa0qVLK9Yk6dJLL03WMXyx5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnbwOpU29L0tNP\nP52sp079PXPmzOS6u3btStbL5XKyPnLkyGR9wYIFFWt5nzFgevDmYusCQRF+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFCM87eBvHH+o0eP1nzfW7ZsSdZfe+21ZH3Pnj3Jupkl6+vXr69YW7FiRXLd+fPnJ+vj\nxo1L1pHGnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgsod5zeztZIWSDri7tdmyx6UtExSX3azle7+\narOaHO5eeumlZP3MmTPJurtXrD3xxBPJdfOOx7/88suT9YsuuihZ/+ijjyrWlixZkly3s7MzWd+w\nYUOyftNNN1WsjRrFR1yq2fOvkzRvkOW/cffp2RfBB4aY3PC7+5uSPmlBLwBaqJ7X/HeZ2dtmttbM\n+JwlMMTUGv5Vkr4nabqkg5J+VemGZrbczMpmVu7r66t0MwAtVlP43f2wu59z9/OSVkuakbhtj7uX\n3L3U0dFRa58AGqym8JvZpAFXfyLp3ca0A6BVqhnq2yDph5ImmNl+SQ9I+qGZTZfkkvZJ+nkTewTQ\nBLnhd/fuQRY/24Rehq284/UfeeSRuu4/dUz91VdfnVz3qaeeStbnzJmTrOd9TuDzzz+vWFu5cmVy\n3dWrVyfr3d2D/Wn+v82bN1eszZhR8ZVqGHzCDwiK8ANBEX4gKMIPBEX4gaAIPxAUxzW2QGq4S6rv\n1NySdPHFF1esrVq1KrnujTfemKyPHj26pp4uGDNmTMXaPffck1x3x44dyfr27duT9dT04wz1secH\nwiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAY52+A8+fPJ+u7d+9O1s+dO5esT5w4MVm///77K9Zmz56d\nXHfEiOL+/0+aNClZf+GFF5L1K6+8Mlkvl8sVa8uWLUuumzf1+HDAnh8IivADQRF+ICjCDwRF+IGg\nCD8QFOEHgmKcvwGOHTuWrOedgjrvcwJ5p99etGhRxVqR4/h58sbSU+cCkPI/H5E63j81rbnEOD+A\nYYzwA0ERfiAowg8ERfiBoAg/EBThB4LKHec3s6mSnpPUKckl9bj7k2Y2XtLvJHVJ2ifpDnf/W/Na\nbV8bN25M1p9//vlkPW8s/uGHH07Wx44dW7HWzuPZeb199tlnLeokpmr2/Gcl/dLdr5E0S9IKM7tG\n0r2Strr7FZK2ZtcBDBG54Xf3g+6+M7t8QtL7kiZLWihpfXaz9ZJub1aTABrvG73mN7MuSd+XtE1S\np7sfzEqH1P+yAMAQUXX4zezbkn4v6RfufnxgzftfvA36As7MlptZ2czKfX19dTULoHGqCr+ZjVZ/\n8H/r7puyxYfNbFJWnyTpyGDrunuPu5fcvdTR0dGIngE0QG74rf/t4Gclve/uvx5Q2iJpaXZ5qaSX\nG98egGap5pDeH0j6qaR3zOzCnMcrJT0u6UUzu1PSXyTd0ZwW20PqsNsXX3wxue6ZM2fqeuy800yn\nTs89fvz45Lp5U3TPmjUrWe/sTL/VkxpK/Pjjj5PrLl68OFnP093dXbHWzoc6t0pu+N29V1Kl3+CP\nGtsOgFbh3x8QFOEHgiL8QFCEHwiK8ANBEX4gKE7d3QCffvppU+9/7969yXpqLP2WW25JrjtlypRk\nPXW4sCSdPXs2WT906FDF2gMPPJBcd+fOncl63hTdt956a7IeHXt+ICjCDwRF+IGgCD8QFOEHgiL8\nQFCEHwiKcf4qpY7/XrNmTXLdZ555Jllft25dsj5u3Lhk/dFHH61Yyzv9dWoaa0nq6elJ1nt7e5P1\nPXv2VKzlTbF91VVXJetvvPFGsp633aJjzw8ERfiBoAg/EBThB4Ii/EBQhB8IivADQVneNMmNVCqV\nvFwut+zxWiV1Tn9JOnjwYLKeN5596tSpZD01E9LJkyeT654+fTpZz/vZ8uojR46sWJs5c2Zy3dTn\nFyRpzpw5yXqR048XpVQqqVwuV/WDs+cHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByj+c3s6mSnpPU\nKckl9bj7k2b2oKRlkvqym65091eb1Wg7y5vr/bLLLkvWX3nllWT97rvvTtZTx8VPnDgxuW69JkyY\nkKw/9thjFWvXX399ct1RozjdRDNVs3XPSvqlu+80s+9I2mFmr2e137j7vzevPQDNkht+dz8o6WB2\n+YSZvS9pcrMbA9Bc3+g1v5l1Sfq+pG3ZorvM7G0zW2tmg54zycyWm1nZzMp9fX2D3QRAAaoOv5l9\nW9LvJf3C3Y9LWiXpe5Kmq/+Zwa8GW8/de9y95O6l1GfQAbRWVeE3s9HqD/5v3X2TJLn7YXc/5+7n\nJa2WNKN5bQJotNzwW/+hUc9Ket/dfz1g+aQBN/uJpHcb3x6AZqnm3f4fSPqppHfMbFe2bKWkbjOb\nrv7hv32Sft6UDoeBvENLb7755mR9x44djWynpfKGQVGcat7t75U02F9vyDF9YLjg3zIQFOEHgiL8\nQFCEHwiK8ANBEX4gKI6ZHAIYK0cz8FcFBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0G1dIpuM+uT9JcB\niyZIOtqyBr6Zdu2tXfuS6K1WjeztH929qvPltTT8X3tws7K7lwprIKFde2vXviR6q1VRvfG0HwiK\n8ANBFR3+noIfP6Vde2vXviR6q1UhvRX6mh9AcYre8wMoSCHhN7N5ZvaBme0xs3uL6KESM9tnZu+Y\n2S4zKxfcy1ozO2Jm7w5YNt7MXjez3dn3QadJK6i3B83sQLbtdpnZbQX1NtXM/svM/mxm75nZ3dny\nQrddoq9CtlvLn/ab2UhJH0qaK2m/pO2Sut39zy1tpAIz2yep5O6Fjwmb2c2STkp6zt2vzZb9m6RP\n3P3x7B/nOHe/p016e1DSyaJnbs4mlJk0cGZpSbdL+pkK3HaJvu5QAdutiD3/DEl73H2vu5+WtFHS\nwgL6aHvu/qakT76yeKGk9dnl9er/42m5Cr21BXc/6O47s8snJF2YWbrQbZfoqxBFhH+ypL8OuL5f\n7TXlt0v6o5ntMLPlRTcziM5s2nRJOiSps8hmBpE7c3MrfWVm6bbZdrXMeN1ovOH3dbPd/Z8lzZe0\nInt625a8/zVbOw3XVDVzc6sMMrP03xW57Wqd8brRigj/AUlTB1yfki1rC+5+IPt+RNJmtd/sw4cv\nTJKafT9ScD9/104zNw82s7TaYNu104zXRYR/u6QrzOy7ZvYtSYskbSmgj68xszHZGzEyszGSfqz2\nm314i6Sl2eWlkl4usJcvaZeZmyvNLK2Ct13bzXjt7i3/knSb+t/x/19J/1pEDxX6ulzSn7Kv94ru\nTdIG9T8NPKP+90bulPQPkrZK2i3pPyWNb6Pe/kPSO5LeVn/QJhXU22z1P6V/W9Ku7Ou2orddoq9C\nthuf8AOC4g0/ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB/R+a36w/tH2XnQAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWsq9ApNPyaz",
        "colab_type": "text"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3aLYGg2P16O",
        "colab_type": "code",
        "outputId": "8e0b9a72-2d6f-4a41-8192-400accc776c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "#test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# EMNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 47 classes: 10 digits, 26 letters, and 11 capital letters \n",
        "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
        "\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# L1 Img Input shape=(n, 28, 28, 1)\n",
        "# Conv = (n, 28, 28, 32)\n",
        "# Pool = (n, 14, 14, 32)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 Img Input shape=(n, 14, 14, 32)\n",
        "# Conv = (n, 14, 14, 64)\n",
        "# Pool = (n, 7, 7, 64)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "# L3 Img Input shape=(n, 7, 7, 64)\n",
        "# Conv = (n, 7, 7, 128)\n",
        "# Pool = (n, 4, 4, 128)\n",
        "# Reshape = (n, 4 * 4 * 128) \n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 Fully Connected (FC) 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128*4*4, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4)+b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final Fully Connected (FC) 625 inputs -> numClasses outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, numClasses], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([numClasses]))\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# cost & train\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test model\n",
        "    is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: test_data, Y: test_labels, keep_prob: 1}))\n",
        "    \n",
        "\n",
        "    # Get Label and predict\n",
        "    r = random.randint(0, numTest - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(test_labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(logits, 1), feed_dict={X: test_data[r : r + 1], keep_prob: 1}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        test_data[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "numTrain:  240000\n",
            "Epoch: 0001, Cost: 0.132140247\n",
            "Epoch: 0002, Cost: 0.040341742\n",
            "Epoch: 0003, Cost: 0.032980858\n",
            "Epoch: 0004, Cost: 0.029762897\n",
            "Epoch: 0005, Cost: 0.027134110\n",
            "Epoch: 0006, Cost: 0.024512553\n",
            "Epoch: 0007, Cost: 0.023280032\n",
            "Epoch: 0008, Cost: 0.021658956\n",
            "Epoch: 0009, Cost: 0.021289860\n",
            "Epoch: 0010, Cost: 0.020385694\n",
            "Epoch: 0011, Cost: 0.019163505\n",
            "Epoch: 0012, Cost: 0.020146366\n",
            "Epoch: 0013, Cost: 0.017946620\n",
            "Epoch: 0014, Cost: 0.018162947\n",
            "Epoch: 0015, Cost: 0.017307029\n",
            "Learning finished\n",
            "Accuracy:  0.996125\n",
            "Label:  [2]\n",
            "Prediction:  [2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbi2dPKW5tgU",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EzV1-qNFYsx",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWD7LYt4F9zW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HocfdIQaFaWG",
        "colab_type": "text"
      },
      "source": [
        "The process of importing training data set and test data set is the same as softmax algorithm above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6kbb3xdGBMo",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odwmf_NJOpjM",
        "colab_type": "text"
      },
      "source": [
        "In this algorith, we use tensorflow layer. So, we use tf.layers.conv2d instead of tf.nn.conv2d as an example. This is because we realize tensorflow layer is easier to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWjryvd5TLNk",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier_files/stackingclassification_overview.png)\n",
        "\n",
        "* Classification models: We train several data set\n",
        "> When new data are added\n",
        "* Predictions: Each classes make each prediction value, which is accuracy\n",
        "* Finally, predicted values are combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzmSCRdYPINa",
        "colab_type": "text"
      },
      "source": [
        "### Convolutaional/Pooling layer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hgeRC6NPOP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional Layer 1 and Pooling Layer 1\n",
        "conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blTni4VDPNe-",
        "colab_type": "text"
      },
      "source": [
        "tf.layers.conv2d\n",
        "* input: An image 28x28\n",
        "* filter: We use 32 filters.\n",
        "* kernel_size: This is the filter size. We use 3x3 filter\n",
        "* padding=\"SAME\":  Input image size and output image size are identical regardless of image size.\n",
        "* activation: We use ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1IJwEuzQBWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=\"SAME\", strides=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MokzOZoQB9f",
        "colab_type": "text"
      },
      "source": [
        "tf.layers.max_pooling2d\n",
        "* input: we use previous layer, which is conv1\n",
        "* pool_size: We use 2x2 filter size. Since pool size is 2x2, original images will decrease. For instance, the image is 28x28 and modified image is 14x14 due to the stride, which is the filter. \n",
        "* strides: The number of steps to move forward at once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fw1Mpg9Qt6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, training=self.training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM-TqdAJQvFx",
        "colab_type": "text"
      },
      "source": [
        "tf.layers.dropout\n",
        "* input: We use previous pooling layer, which is pool1\n",
        "* rate: We would like to keep 30% of networks to apply dropout\n",
        "* training: Wether to either train or test. So, this is boolean value. If we are testing, dropout rate is automatically 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSbexWtLRVcV",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional/Pooling layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwM-Ifs6RZrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional Layer 2 and Pooling Layer 2\n",
        "conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, training=self.training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfwhu3CBRbdB",
        "colab_type": "text"
      },
      "source": [
        "It is the same as convolutional layer 1 and pooling layer 1. However, the number of filters is different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnDMTQBbRvBq",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional/Pooling layer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMu0xKZ8Rz7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional Layer 3 and Pooling Layer 3\n",
        "conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, training=self.training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yT2LuTlRxWu",
        "colab_type": "text"
      },
      "source": [
        "It is the same as convolutional layer 2 and pooling layer 2. However, the number of filters is different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOUAp8mpR41w",
        "colab_type": "text"
      },
      "source": [
        "## Fully Connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE85NDhHR8VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense Layer with Relu\n",
        "flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkhWFX0_R7U2",
        "colab_type": "text"
      },
      "source": [
        "Before we get into the Fully Connected layer, we should reshape array size. In pooling layer3, there are 128 different images and the image size is 4*4. So, array shape should be 128x4x4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHdxfQbuSan2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n",
        "dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=self.training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDefqdr_SbRs",
        "colab_type": "text"
      },
      "source": [
        "We use tf.layers.dense in order for Fully Connected layer\n",
        "* input: Reshaped array\n",
        "* units: A number of outputs\n",
        "* activation: We use ReLU\n",
        "\n",
        "Dropout is the same as above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bez_ikNWWV4q",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqn2985KT58z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "num_models = 4\n",
        "for m in range(num_models):\n",
        "    models.append(Model(sess, \"model\" + str(m)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8TY8Df1T6sf",
        "colab_type": "text"
      },
      "source": [
        "We use 4 models. So we make 4 models.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkaFLWV4UTzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train each model\n",
        "for m_idx, m in enumerate(models):\n",
        "  c, _ = m.train(batch_xs, batch_ys)\n",
        "  avg_cost_list[m_idx] += c / num_iterations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4WXtjUZUUWX",
        "colab_type": "text"
      },
      "source": [
        "We train each models. This is a difference between CNN above and ensemble. avg_cost_list has each accuracy value of each models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIfPrndhVG-d",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV3qaI7wVIal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test & accuracy\n",
        "predictions = np.zeros([numTest, numClasses])\n",
        "for m_idx, m in enumerate(models):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(test_data, test_labels))\n",
        "    p = m.predict(test_data)\n",
        "    predictions += p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrjBbUB6VI4X",
        "colab_type": "text"
      },
      "source": [
        "We make 47 arrays becaue of the number of outputs. Each prediction is added, which is predictions in the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty2fzjZLVuYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smT6JYccVqbi",
        "colab_type": "text"
      },
      "source": [
        "We compare prediction value with test label. And then, we can calculate the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWCz40T1S2th",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a9o8crXi9yi",
        "colab_type": "text"
      },
      "source": [
        "### Alphabet/Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYLhyvP_RPdK",
        "colab_type": "code",
        "outputId": "a1128e0c-6b01-4c8c-98f4-941403f7f4e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "#train = pd.read_csv('emnist/emnist-digits-train.csv', header=None)\n",
        "\n",
        "test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "#test = pd.read_csv('emnist/emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._runEnsemble()\n",
        "\n",
        "    def _runEnsemble(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # EMNIST data image of shape 28 * 28 = 784\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "            # 47 classes: 10 digits, 26 letters, and 11 capital letters\n",
        "            self.Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "            # Input Image\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "\n",
        "            # Convolutional Layer 1 and Pooling Layer 1\n",
        "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "            dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolutional Layer 2 and Pooling Layer 2\n",
        "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "            dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolutional Layer 3 and Pooling Layer 3\n",
        "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "            dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, training=self.training)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
        "            dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n",
        "            dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=self.training)\n",
        "\n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> outputs\n",
        "            self.logits = tf.layers.dense(inputs=dropout4, units=numClasses)\n",
        "\n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: x_data, self.Y: y_data, self.training: training})\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "\n",
        "models = []\n",
        "num_models = 4\n",
        "for m in range(num_models):\n",
        "    models.append(Model(sess, \"model\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "\n",
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "    avg_cost_list = np.zeros(len(models))\n",
        "    for i in range(num_iterations):\n",
        "        batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "\n",
        "        # Train each model\n",
        "        for m_idx, m in enumerate(models):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / num_iterations\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "\n",
        "# Test & accuracy\n",
        "predictions = np.zeros([numTest, numClasses])\n",
        "for m_idx, m in enumerate(models):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(test_data, test_labels))\n",
        "    p = m.predict(test_data)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Final accuracy:', sess.run(ensemble_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-4aeab7799e58>:79: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-7-4aeab7799e58>:80: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-7-4aeab7799e58>:81: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-7-4aeab7799e58>:95: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-7-4aeab7799e58>:102: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Learning Started!\n",
            "numTrain:  112800\n",
            "Epoch: 0001 cost = [0.93694787 0.96545307 0.96958477 0.95645672]\n",
            "Epoch: 0002 cost = [0.51001651 0.52268768 0.51578384 0.51749801]\n",
            "Epoch: 0003 cost = [0.44954652 0.46018538 0.45869213 0.45723559]\n",
            "Epoch: 0004 cost = [0.42094525 0.42600334 0.42507715 0.42196886]\n",
            "Epoch: 0005 cost = [0.39802434 0.40422068 0.4024648  0.40161107]\n",
            "Epoch: 0006 cost = [0.38610352 0.3868933  0.38614775 0.38660766]\n",
            "Epoch: 0007 cost = [0.37033975 0.37395777 0.3706577  0.37284704]\n",
            "Epoch: 0008 cost = [0.35975115 0.36336663 0.35888761 0.36276961]\n",
            "Epoch: 0009 cost = [0.35073441 0.3545419  0.35260872 0.35529837]\n",
            "Epoch: 0010 cost = [0.34365498 0.34878156 0.34508266 0.34724383]\n",
            "Epoch: 0011 cost = [0.33927261 0.34045809 0.33935128 0.34327952]\n",
            "Epoch: 0012 cost = [0.3334537  0.3365682  0.33567896 0.33747963]\n",
            "Epoch: 0013 cost = [0.32677686 0.32869293 0.32912845 0.33117159]\n",
            "Epoch: 0014 cost = [0.32393402 0.32300278 0.32438993 0.32769079]\n",
            "Epoch: 0015 cost = [0.32108156 0.32197558 0.31911203 0.32423962]\n",
            "Learning Finished!\n",
            "0 Accuracy: 0.89079785\n",
            "1 Accuracy: 0.89143616\n",
            "2 Accuracy: 0.8932447\n",
            "3 Accuracy: 0.8907447\n",
            "Final accuracy: 0.89521277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYj6K6vQjCkC",
        "colab_type": "text"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axRZTtL5gSYU",
        "colab_type": "code",
        "outputId": "76568e39-756c-49a8-c1b8-7f08a6c7b991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
        "train = pd.read_csv('emnist-digits-train.csv', header=None)\n",
        "\n",
        "#test = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
        "test = pd.read_csv('emnist-digits-test.csv', header=None)\n",
        "\n",
        "#train.head()\n",
        "\n",
        "# Number of Train\n",
        "numTrain = len(train)\n",
        "# Number of Test\n",
        "numTest = len(test)\n",
        "# Number of Classes\n",
        "numClasses = len(train[0].unique())\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data = train.iloc[:, 1:]\n",
        "train_labels = train.iloc[:, 0]\n",
        "test_data = test.iloc[:, 1:]\n",
        "test_labels = test.iloc[:, 0]\n",
        "\n",
        "# one-hot encoding\n",
        "train_labels = pd.get_dummies(train_labels)\n",
        "test_labels = pd.get_dummies(test_labels)\n",
        "#train_labels.head()\n",
        "\n",
        "train_data = train_data.values\n",
        "train_labels = train_labels.values\n",
        "test_data = test_data.values\n",
        "test_labels = test_labels.values\n",
        "\n",
        "\n",
        "rand = random.randint(0, numTrain - 1)\n",
        "\n",
        "'''\n",
        "plt.imshow(\n",
        "    train_data[rand].reshape([28, 28]), \n",
        "    cmap='Greys'\n",
        "    )\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image.reshape([28 * 28])\n",
        "train_data = np.apply_along_axis(rotate, 1, train_data)/255\n",
        "test_data = np.apply_along_axis(rotate, 1, test_data)/255\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._runEnsemble()\n",
        "\n",
        "    def _runEnsemble(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # EMNIST data image of shape 28 * 28 = 784\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "            # 47 classes: 10 digits, 26 letters, and 11 capital letters\n",
        "            self.Y = tf.placeholder(tf.float32, [None, numClasses])\n",
        "            # Input Image\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "\n",
        "            # Convolutional Layer 1 and Pooling Layer 1\n",
        "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "            dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolutional Layer 2 and Pooling Layer 2\n",
        "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "            dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolutional Layer 3 and Pooling Layer 3\n",
        "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=\"SAME\", strides=2)\n",
        "            dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, training=self.training)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
        "            dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n",
        "            dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=self.training)\n",
        "\n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> outputs\n",
        "            self.logits = tf.layers.dense(inputs=dropout4, units=numClasses)\n",
        "\n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: x_data, self.Y: y_data, self.training: training})\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "\n",
        "models = []\n",
        "num_models = 4\n",
        "for m in range(num_models):\n",
        "    models.append(Model(sess, \"model\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "print('numTrain: ', numTrain)\n",
        "num_iterations = int( numTrain / batch_size)\n",
        "\n",
        "\n",
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "    avg_cost_list = np.zeros(len(models))\n",
        "    for i in range(num_iterations):\n",
        "        batch_xs, batch_ys = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n",
        "\n",
        "        # Train each model\n",
        "        for m_idx, m in enumerate(models):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / num_iterations\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "\n",
        "# Test & accuracy\n",
        "predictions = np.zeros([numTest, numClasses])\n",
        "for m_idx, m in enumerate(models):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(test_data, test_labels))\n",
        "    p = m.predict(test_data)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(test_labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Final accuracy:', sess.run(ensemble_accuracy))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-0917ef1bdd41>:79: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-9-0917ef1bdd41>:80: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-9-0917ef1bdd41>:81: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-9-0917ef1bdd41>:95: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-9-0917ef1bdd41>:102: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Learning Started!\n",
            "numTrain:  240000\n",
            "Epoch: 0001 cost = [0.10468324 0.10184871 0.10251109 0.10327382]\n",
            "Epoch: 0002 cost = [0.03955471 0.03966181 0.04028189 0.03961207]\n",
            "Epoch: 0003 cost = [0.03291164 0.03285973 0.03267252 0.03280353]\n",
            "Epoch: 0004 cost = [0.02949152 0.02939486 0.02953999 0.02957595]\n",
            "Epoch: 0005 cost = [0.02754066 0.02755699 0.02666415 0.02744095]\n",
            "Epoch: 0006 cost = [0.02618295 0.02627841 0.02529343 0.0257399 ]\n",
            "Epoch: 0007 cost = [0.02505093 0.02504658 0.02427934 0.0251797 ]\n",
            "Epoch: 0008 cost = [0.02341027 0.02411584 0.02372315 0.02406025]\n",
            "Epoch: 0009 cost = [0.02358986 0.02329099 0.02328037 0.02309247]\n",
            "Epoch: 0010 cost = [0.02355568 0.02233755 0.0228784  0.02300813]\n",
            "Epoch: 0011 cost = [0.02229733 0.02200331 0.0218251  0.02277981]\n",
            "Epoch: 0012 cost = [0.02233569 0.02140354 0.02109605 0.02199041]\n",
            "Epoch: 0013 cost = [0.02120303 0.02134857 0.02285071 0.02114544]\n",
            "Epoch: 0014 cost = [0.02178189 0.02170915 0.02136285 0.02185103]\n",
            "Epoch: 0015 cost = [0.02130419 0.02103714 0.02110437 0.02036902]\n",
            "Learning Finished!\n",
            "0 Accuracy: 0.9966\n",
            "1 Accuracy: 0.996625\n",
            "2 Accuracy: 0.996575\n",
            "3 Accuracy: 0.996425\n",
            "Final accuracy: 0.997225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT0l5r1Bb-Hj",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWtbM0NDdZwR",
        "colab_type": "text"
      },
      "source": [
        "||Alphabet/Digits|Digits|\n",
        "|--|:----------------|-------|\n",
        "|Logistic Regression|0.6085106|0.7697|\n",
        "|Neural Networks|0.6993085|0.9823|\n",
        "|Xavier|0.8368085|0.988775|\n",
        "|Dropout|0.84957445|0.990575|\n",
        "|CNN|0.89090425|0.996125|\n",
        "|Ensemble|0.89521277|0.997225|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VemU-1Gi9B9V",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "\n",
        "1.   Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre ́ van Schaik, \"EMNIST: an extension of MNIST to handwritten letters\", March 2017\n",
        "2.  http://cs231n.github.io/convolutional-networks/\n",
        "3. Diederik P. Kingma, Jimmy Lei Ba, \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\", ICLR 2015\n",
        "4.  https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
        "5. https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
        "6. Xavier Glorot, Yoshua Bengio, \"Understanding the difficulty of training deep feedforward neural networks\", 2010\n",
        "7. https://github.com/google/prettytensor/blob/a69f13998258165d6682a47a931108d974bab05e/prettytensor/layers.py\n",
        "8. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfiting\", Journal of Machine Learning Research, May 2014\n",
        "9. Shuo Yang, Ping Luo, Chen Change Loy, Kenneth W. Shu, Xiaoou Tang, \"Deep Representation Learning with Target Coding\", AAAI Conference on Artificial Intelligence, 2015\n",
        "10. Tang, J., S. Alelyani, and H. Liu. \"Data Classification: Algorithms and Applications.\" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500"
      ]
    }
  ]
}